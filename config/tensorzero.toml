# ┌────────────────────────────────────────┐
# │         REQUIRED SERVER CONFIG         │
# └────────────────────────────────────────┘
[gateway]
bind_address = "0.0.0.0:3000"



# A function defines the task we're tackling (e.g. generating a haiku)...
[functions.generate_haiku]
type = "chat"

[functions.generate_short_story]
type = "chat"

# ... and a variant is one of many implementations we can use to tackle it (a choice of prompt, model, etc.).
# Since we only have one variant for this function, the gateway will always select it.

[functions.generate_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "gpt-4o-mini"

[functions.generate_short_story.variants."gpt-4o"]
type = "chat_completion"
model = "gpt-4o"
weight = 0.25


[functions.generate_short_story.variants.gpt_4o_mini]
type = "chat_completion"
model = "gpt-4o-mini"
weight = 0.25

[functions.generate_short_story.variants.llama_3_1_8b]
type = "chat_completion"
model = "llama_model"
weight = 0.25

[functions.generate_short_story.variants.llama_70b]
type = "chat_completion"
model = "llama_70b"
weight = 0.25

[functions.generate_short_story.variants.mistral_7b]
type = "chat_completion"
model = "mistral_7b"
weight = 0.25

[functions.generate_short_story.variants.nemotron_70b]
type = "chat_completion"
model = "nemotron_70b"
weight = 0.25

[functions.generate_short_story.variants.mixtral_8x7b]
type = "chat_completion"
model = "mixtral_8x7b"
weight = 0.25

# Add a coding-specific function
[functions.generate_code]
type = "chat"

[functions.generate_code.variants.code_llama]
type = "chat_completion"
model = "code_llama"
weight = 1.0


[metrics.short_story_rating]
type = "float"
optimize = "max"
level = "inference"

[models.gpt-4o]
routing = ["openai/gpt-4o"]

[models.gpt-4o.providers."openai/gpt-4o"]
type = "openai"
model_name = "gpt-4o"
api_key_location = "env::OPENAI_API_KEY"

[models.llama_model]
routing = ["my_nim_provider/llama-8b"]

[models.llama_model.providers."my_nim_provider/llama-8b"]
type = "nvidia_nim"
model_name = "meta/llama-3.1-8b-instruct"
api_key_location = "env::NVIDIA_API_KEY"

# Add Llama 70B model
[models.llama_70b]
routing = ["my_nim_provider/llama-70b"]

[models.llama_70b.providers."my_nim_provider/llama-70b"]
type = "openai"
model_name = "meta/llama-3.1-70b-instruct"
api_base = "https://integrate.api.nvidia.com/v1"
api_key_location = "env::NVIDIA_API_KEY"

# Add Mistral 7B model (using available version)
[models.mistral_7b]
routing = ["my_nim_provider/mistral-7b"]

[models.mistral_7b.providers."my_nim_provider/mistral-7b"]
type = "openai"
model_name = "mistralai/mistral-7b-instruct-v0.3"
api_base = "https://integrate.api.nvidia.com/v1"
api_key_location = "env::NVIDIA_API_KEY"

# Add Code Llama for coding tasks
[models.code_llama]
routing = ["my_nim_provider/code-llama"]

[models.code_llama.providers."my_nim_provider/code-llama"]
type = "openai"
model_name = "meta/codellama-70b"
api_base = "https://integrate.api.nvidia.com/v1"
api_key_location = "env::NVIDIA_API_KEY"

# Add Nemotron model (NVIDIA's own model)
[models.nemotron_70b]
routing = ["my_nim_provider/nemotron"]

[models.nemotron_70b.providers."my_nim_provider/nemotron"]
type = "openai"
model_name = "nvidia/llama-3.1-nemotron-70b-instruct"
api_base = "https://integrate.api.nvidia.com/v1"
api_key_location = "env::NVIDIA_API_KEY"

# Add Mixtral 8x7B (more powerful Mistral)
[models.mixtral_8x7b]
routing = ["my_nim_provider/mixtral"]

[models.mixtral_8x7b.providers."my_nim_provider/mixtral"]
type = "openai"
model_name = "mistralai/mixtral-8x7b-instruct-v0.1"
api_base = "https://integrate.api.nvidia.com/v1"
api_key_location = "env::NVIDIA_API_KEY"

# Add GPT-4o-mini model
[models.gpt-4o-mini]
routing = ["openai/gpt-4o-mini"]

[models.gpt-4o-mini.providers."openai/gpt-4o-mini"]
type = "openai"
model_name = "gpt-4o-mini"
api_key_location = "env::OPENAI_API_KEY"
